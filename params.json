{
  "name": "Practical Machine Learning - Assignment",
  "tagline": "Coursera Final Project",
  "body": "# Practical Machine Learning - Final Project\r\nTim Terry  \r\nJune 27, 2016  \r\n\r\n\r\n\r\n## Synopsis\r\n\r\nUsing devices such as Jawbone Up, Nike FuelBand, and Fitbit, it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self-movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. \r\n\r\nOne thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, the goal is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. \r\n\r\nMore information is available from the website here:\r\n\r\nhttp://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).\r\n\r\nThe training data for this project are available here:\r\n\r\nhttps://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\r\n\r\nThe test data are available here:\r\n\r\nhttps://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\r\n\r\nThe data for this project come from this source:\r\n\r\nhttp://groupware.les.inf.puc-rio.br/har. \r\n\r\n\r\n```r\r\nlibrary(caret)\r\nlibrary(rattle)\r\nlibrary(rpart.plot)\r\nlibrary(randomForest)\r\nlibrary(rpart)\r\n```\r\n\r\n## Data Gathering\r\n\r\nThe files were downloaded from the above sites prior to starting the analysis and my working directory was set to their folder location.\r\n\r\n\r\n```r\r\npml_train <- read.csv(\"pml-training.csv\", sep = \",\", na.strings = c(\"NA\", \"#DIV/0!\", \"\"), header=TRUE)\r\npml_test  <- read.csv(\"pml-testing.csv\", sep = \",\", na.strings = c(\"NA\", \"#DIV/0!\", \"\"), header=TRUE)\r\n\r\ndim(pml_train); dim(pml_test);\r\n```\r\n\r\n```\r\n## [1] 19622   160\r\n```\r\n\r\n```\r\n## [1]  20 160\r\n```\r\n\r\nThe \"pml-testing.csv\" contains the 20 samples that the final predictive model will be applied to. I want to confirm that the datasets are identical with the exception of the last column which contains our outcome.\r\n\r\n\r\n```r\r\ncnames_train <- colnames(pml_train)\r\ncnames_test <- colnames(pml_test)\r\n\r\nall.equal(cnames_train[1:159], cnames_test[1:159])\r\n```\r\n\r\n```\r\n## [1] TRUE\r\n```\r\n\r\n## Data Processing\r\n\r\nFor Cross-Validation purposes, I'm going to split the Training dataset into two tables, one for training my model(s) and the second for testing the model(s). All data cleaning or processing that is performed on the training set (myTrain) will also be performed on the testing set (myTest).\r\n\r\n\r\n```r\r\nset.seed(215)\r\n\r\ninTrain <- createDataPartition(y=pml_train$classe, p=0.7, list=FALSE)\r\nmyTrain <- pml_train[inTrain,]\r\nmyTest <- pml_train[-inTrain,]\r\n\r\ndim(myTrain); dim(myTest);\r\n```\r\n\r\n```\r\n## [1] 13737   160\r\n```\r\n\r\n```\r\n## [1] 5885  160\r\n```\r\n\r\nDuring my initial data analysis it was discovered that there were a large set of variables that contained NAs. I want to remove these from my analysis. Any variable that is over 50% NA will be removed.\r\n\r\n\r\n```r\r\nremNA <- sapply(myTrain, function(x) mean(is.na(x))) > 0.5\r\ntable(remNA)\r\n```\r\n\r\n```\r\n## remNA\r\n## FALSE  TRUE \r\n##    60   100\r\n```\r\n\r\n100 variables will be removed. \r\n\r\n\r\n```r\r\nmyTrain <- myTrain[, remNA==FALSE]\r\nmyTest <- myTest[, remNA==FALSE]\r\n\r\ndim(myTrain); dim(myTest);\r\n```\r\n\r\n```\r\n## [1] 13737    60\r\n```\r\n\r\n```\r\n## [1] 5885   60\r\n```\r\n\r\nNext, we will look for variables that have near zero variance.\r\n\r\n\r\n```r\r\nremNZV <- nearZeroVar(myTrain)\r\nprint(remNZV)\r\n```\r\n\r\n```\r\n## [1] 6\r\n```\r\n\r\nThis is the $new_window factor variable(yes/no)\r\n\r\n\r\n```r\r\nmyTrain <- myTrain[,-remNZV]\r\nmyTest <- myTest[, -remNZV]\r\ndim(myTrain); dim(myTest);\r\n```\r\n\r\n```\r\n## [1] 13737    59\r\n```\r\n\r\n```\r\n## [1] 5885   59\r\n```\r\n\r\nThe first 6 variables in the dataset contain timestamps, user names, and sample identifiers. These can be removed.\r\n\r\n\r\n```r\r\nmyTrain <- myTrain[, -(1:6)]\r\nmyTest <- myTest[, -(1:6)]\r\n\r\ndim(myTrain); dim(myTest);\r\n```\r\n\r\n```\r\n## [1] 13737    53\r\n```\r\n\r\n```\r\n## [1] 5885   53\r\n```\r\n\r\n## Model Development\r\n\r\n### Decision Trees\r\n\r\nMy first model attempt is a Decision Tree.\r\n\r\n\r\n```r\r\ndtFit1 <- rpart(classe ~., data=myTrain, method=\"class\")\r\ndfPredict1 <- predict(dtFit1, myTest, type=\"class\")\r\nconfusionMatrix(dfPredict1, myTest$classe)\r\n```\r\n\r\n```\r\n## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction    A    B    C    D    E\r\n##          A 1490  254   30  103   43\r\n##          B   29  585   87   37   68\r\n##          C   48  155  818  122  143\r\n##          D   71   85   67  638   56\r\n##          E   36   60   24   64  772\r\n## \r\n## Overall Statistics\r\n##                                           \r\n##                Accuracy : 0.7312          \r\n##                  95% CI : (0.7197, 0.7425)\r\n##     No Information Rate : 0.2845          \r\n##     P-Value [Acc > NIR] : < 2.2e-16       \r\n##                                           \r\n##                   Kappa : 0.6585          \r\n##  Mcnemar's Test P-Value : < 2.2e-16       \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity            0.8901  0.51361   0.7973   0.6618   0.7135\r\n## Specificity            0.8979  0.95343   0.9037   0.9433   0.9617\r\n## Pos Pred Value         0.7760  0.72581   0.6361   0.6957   0.8075\r\n## Neg Pred Value         0.9536  0.89092   0.9548   0.9344   0.9371\r\n## Prevalence             0.2845  0.19354   0.1743   0.1638   0.1839\r\n## Detection Rate         0.2532  0.09941   0.1390   0.1084   0.1312\r\n## Detection Prevalence   0.3263  0.13696   0.2185   0.1558   0.1624\r\n## Balanced Accuracy      0.8940  0.73352   0.8505   0.8026   0.8376\r\n```\r\n\r\nThe Decision Tree resulted in an accuracy rating of 73.12% with a 95% Confidence Interval from 71.97% to 74.25%. I would like to have an Out of Sample error rate < 1%.\r\n\r\n### Random Forest\r\n\r\nMy second model will be a Random Forest without pre-processing to see if we can get a higher accuracy result.\r\n\r\n\r\n```r\r\nrfFit1 <- randomForest(classe ~., data=myTrain, method=\"class\")\r\nrfPredict1 <- predict(rfFit1, myTest, type=\"class\")\r\nconfusionMatrix(rfPredict1, myTest$classe)\r\n```\r\n\r\n```\r\n## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction    A    B    C    D    E\r\n##          A 1674    6    0    0    0\r\n##          B    0 1131    5    0    0\r\n##          C    0    2 1020    6    0\r\n##          D    0    0    1  956    7\r\n##          E    0    0    0    2 1075\r\n## \r\n## Overall Statistics\r\n##                                           \r\n##                Accuracy : 0.9951          \r\n##                  95% CI : (0.9929, 0.9967)\r\n##     No Information Rate : 0.2845          \r\n##     P-Value [Acc > NIR] : < 2.2e-16       \r\n##                                           \r\n##                   Kappa : 0.9938          \r\n##  Mcnemar's Test P-Value : NA              \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity            1.0000   0.9930   0.9942   0.9917   0.9935\r\n## Specificity            0.9986   0.9989   0.9984   0.9984   0.9996\r\n## Pos Pred Value         0.9964   0.9956   0.9922   0.9917   0.9981\r\n## Neg Pred Value         1.0000   0.9983   0.9988   0.9984   0.9985\r\n## Prevalence             0.2845   0.1935   0.1743   0.1638   0.1839\r\n## Detection Rate         0.2845   0.1922   0.1733   0.1624   0.1827\r\n## Detection Prevalence   0.2855   0.1930   0.1747   0.1638   0.1830\r\n## Balanced Accuracy      0.9993   0.9960   0.9963   0.9950   0.9966\r\n```\r\n\r\nThe Random Forest model did a significantly better job of predicting the test data with an accuracy rating of 99.51% with a 95% Confidence Interval from 99.29% to 99.67% and an Out of Sample error of 0.49%\r\n\r\nIn an attempt to see if we can improve on that rating I'm going to try pre-processing with Principle Component Analysis.\r\n\r\n\r\n```r\r\nrfFit2 <- randomForest(classe ~., data=myTrain, preProcess=\"pca\", method=\"class\")\r\nrfPredict2 <- predict(rfFit2, myTest, type=\"class\")\r\nconfusionMatrix(rfPredict2, myTest$classe)\r\n```\r\n\r\n```\r\n## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction    A    B    C    D    E\r\n##          A 1674    4    0    0    0\r\n##          B    0 1133    6    0    0\r\n##          C    0    2 1019    6    0\r\n##          D    0    0    1  956    6\r\n##          E    0    0    0    2 1076\r\n## \r\n## Overall Statistics\r\n##                                          \r\n##                Accuracy : 0.9954         \r\n##                  95% CI : (0.9933, 0.997)\r\n##     No Information Rate : 0.2845         \r\n##     P-Value [Acc > NIR] : < 2.2e-16      \r\n##                                          \r\n##                   Kappa : 0.9942         \r\n##  Mcnemar's Test P-Value : NA             \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity            1.0000   0.9947   0.9932   0.9917   0.9945\r\n## Specificity            0.9991   0.9987   0.9984   0.9986   0.9996\r\n## Pos Pred Value         0.9976   0.9947   0.9922   0.9927   0.9981\r\n## Neg Pred Value         1.0000   0.9987   0.9986   0.9984   0.9988\r\n## Prevalence             0.2845   0.1935   0.1743   0.1638   0.1839\r\n## Detection Rate         0.2845   0.1925   0.1732   0.1624   0.1828\r\n## Detection Prevalence   0.2851   0.1935   0.1745   0.1636   0.1832\r\n## Balanced Accuracy      0.9995   0.9967   0.9958   0.9951   0.9970\r\n```\r\n\r\nPrinciple Component Analysis yielded a slightly higher accuracy rating, 99.54%, with a 95% Confidence Interval between 99.33% and 99.70% with an Out of Sample error of 0.46%.\r\n\r\nSince the data in the variables we are testing with have different ranges of values I'd like to Standardize the measures to reduce any impact of highly skewed variables to see if there is an improvement in the model. \r\n\r\n\r\n```r\r\nrfFit3 <- randomForest(classe ~., data=myTrain, preProcess=c(\"center\", \"scale\"), method=\"class\")\r\nrfPredict3 <- predict(rfFit3, myTest, type=\"class\")\r\nconfusionMatrix(rfPredict3, myTest$classe)\r\n```\r\n\r\n```\r\n## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction    A    B    C    D    E\r\n##          A 1674    5    0    0    0\r\n##          B    0 1132    6    0    0\r\n##          C    0    2 1020    5    0\r\n##          D    0    0    0  957    7\r\n##          E    0    0    0    2 1075\r\n## \r\n## Overall Statistics\r\n##                                          \r\n##                Accuracy : 0.9954         \r\n##                  95% CI : (0.9933, 0.997)\r\n##     No Information Rate : 0.2845         \r\n##     P-Value [Acc > NIR] : < 2.2e-16      \r\n##                                          \r\n##                   Kappa : 0.9942         \r\n##  Mcnemar's Test P-Value : NA             \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity            1.0000   0.9939   0.9942   0.9927   0.9935\r\n## Specificity            0.9988   0.9987   0.9986   0.9986   0.9996\r\n## Pos Pred Value         0.9970   0.9947   0.9932   0.9927   0.9981\r\n## Neg Pred Value         1.0000   0.9985   0.9988   0.9986   0.9985\r\n## Prevalence             0.2845   0.1935   0.1743   0.1638   0.1839\r\n## Detection Rate         0.2845   0.1924   0.1733   0.1626   0.1827\r\n## Detection Prevalence   0.2853   0.1934   0.1745   0.1638   0.1830\r\n## Balanced Accuracy      0.9994   0.9963   0.9964   0.9957   0.9966\r\n```\r\n\r\nThe Standardized Random Forest resulted in the same accuracy rating, Confidence Interval, and Out of Sample error rate.\r\n\r\nSince the results of the PCA and Standardized Random Forests are the same and each meets the goal of an Out of Sample Error Rate of < 1%, we can choose either model as our predictor for the test set. In this case I have chosen the Standardized Random Forest model.\r\n\r\n## Results with Test Data\r\n\r\nThe first set of tasks is to apply all of the data cleaning steps from our model building to the test data in pml_test.\r\n\r\n\r\n```r\r\n## Remove NAs\r\npml_test <- pml_test[, remNA==FALSE]\r\n\r\n## Remove Near Zero Variance\r\npml_test <- pml_test[,-remNZV]\r\n\r\n## Remove first 4 variables\r\npml_test <- pml_test[, -(1:6)]\r\n```\r\n\r\nThe final item is to make sure that all of the column classes for the pml_test data match those of the myTrain data.\r\n\r\nThere are three variable in pml_test that have different classes than their counterparts in myTrain. I will coerce them to match.\r\n\r\n\r\n```r\r\n## Coerce from int to num\r\npml_test$magnet_dumbbell_z <- as.numeric(pml_test$magnet_dumbbell_z)\r\npml_test$magnet_forearm_y <- as.numeric(pml_test$magnet_forearm_y)\r\npml_test$magnet_forearm_z <- as.numeric(pml_test$magnet_forearm_z)\r\n```\r\n\r\nRun the prediction against the pml_test data.\r\n\r\n\r\n```r\r\npmlPredict <- predict(rfFit3, newdata=pml_test)\r\n\r\npmlPredResults <- data.frame(problem_id = pml_test$problem_id, predicted = pmlPredict)\r\n```\r\n\r\n### Prediction Results\r\n\r\n\r\n```r\r\nprint(pmlPredResults)\r\n```\r\n\r\n```\r\n##    problem_id predicted\r\n## 1           1         B\r\n## 2           2         A\r\n## 3           3         B\r\n## 4           4         A\r\n## 5           5         A\r\n## 6           6         E\r\n## 7           7         D\r\n## 8           8         B\r\n## 9           9         A\r\n## 10         10         A\r\n## 11         11         B\r\n## 12         12         C\r\n## 13         13         B\r\n## 14         14         A\r\n## 15         15         E\r\n## 16         16         E\r\n## 17         17         A\r\n## 18         18         B\r\n## 19         19         B\r\n## 20         20         B\r\n```\r\n\r\n\r\n\r\n\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}